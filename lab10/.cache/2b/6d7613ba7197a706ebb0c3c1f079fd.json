{"id":"node_modules/@tensorflow/tfjs-core/dist/optimizers/optimizer_constructors.js","dependencies":[{"name":"/home/richardrose/pet-theory/lab10/node_modules/@tensorflow/tfjs-core/dist/optimizers/optimizer_constructors.js.map","includedInParent":true,"mtime":1624262084444},{"name":"/home/richardrose/pet-theory/lab10/node_modules/@tensorflow/tfjs-core/src/optimizers/optimizer_constructors.ts","includedInParent":true,"mtime":1624262084253},{"name":"/home/richardrose/pet-theory/lab10/package.json","includedInParent":true,"mtime":1624262100639},{"name":"/home/richardrose/pet-theory/lab10/node_modules/@tensorflow/tfjs-core/package.json","includedInParent":true,"mtime":1624262086076},{"name":"./adadelta_optimizer","loc":{"line":17,"column":34},"parent":"/home/richardrose/pet-theory/lab10/node_modules/@tensorflow/tfjs-core/dist/optimizers/optimizer_constructors.js","resolved":"/home/richardrose/pet-theory/lab10/node_modules/@tensorflow/tfjs-core/dist/optimizers/adadelta_optimizer.js"},{"name":"./adagrad_optimizer","loc":{"line":18,"column":33},"parent":"/home/richardrose/pet-theory/lab10/node_modules/@tensorflow/tfjs-core/dist/optimizers/optimizer_constructors.js","resolved":"/home/richardrose/pet-theory/lab10/node_modules/@tensorflow/tfjs-core/dist/optimizers/adagrad_optimizer.js"},{"name":"./adam_optimizer","loc":{"line":19,"column":30},"parent":"/home/richardrose/pet-theory/lab10/node_modules/@tensorflow/tfjs-core/dist/optimizers/optimizer_constructors.js","resolved":"/home/richardrose/pet-theory/lab10/node_modules/@tensorflow/tfjs-core/dist/optimizers/adam_optimizer.js"},{"name":"./adamax_optimizer","loc":{"line":20,"column":32},"parent":"/home/richardrose/pet-theory/lab10/node_modules/@tensorflow/tfjs-core/dist/optimizers/optimizer_constructors.js","resolved":"/home/richardrose/pet-theory/lab10/node_modules/@tensorflow/tfjs-core/dist/optimizers/adamax_optimizer.js"},{"name":"./momentum_optimizer","loc":{"line":21,"column":34},"parent":"/home/richardrose/pet-theory/lab10/node_modules/@tensorflow/tfjs-core/dist/optimizers/optimizer_constructors.js","resolved":"/home/richardrose/pet-theory/lab10/node_modules/@tensorflow/tfjs-core/dist/optimizers/momentum_optimizer.js"},{"name":"./rmsprop_optimizer","loc":{"line":22,"column":33},"parent":"/home/richardrose/pet-theory/lab10/node_modules/@tensorflow/tfjs-core/dist/optimizers/optimizer_constructors.js","resolved":"/home/richardrose/pet-theory/lab10/node_modules/@tensorflow/tfjs-core/dist/optimizers/rmsprop_optimizer.js"},{"name":"./sgd_optimizer","loc":{"line":23,"column":29},"parent":"/home/richardrose/pet-theory/lab10/node_modules/@tensorflow/tfjs-core/dist/optimizers/optimizer_constructors.js","resolved":"/home/richardrose/pet-theory/lab10/node_modules/@tensorflow/tfjs-core/dist/optimizers/sgd_optimizer.js"}],"generated":{"js":"\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.OptimizerConstructors = void 0;\n\nvar _adadelta_optimizer = require(\"./adadelta_optimizer\");\n\nvar _adagrad_optimizer = require(\"./adagrad_optimizer\");\n\nvar _adam_optimizer = require(\"./adam_optimizer\");\n\nvar _adamax_optimizer = require(\"./adamax_optimizer\");\n\nvar _momentum_optimizer = require(\"./momentum_optimizer\");\n\nvar _rmsprop_optimizer = require(\"./rmsprop_optimizer\");\n\nvar _sgd_optimizer = require(\"./sgd_optimizer\");\n\n/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nclass OptimizerConstructors {\n  /**\n   * Constructs a `tf.SGDOptimizer` that uses stochastic gradient descent.\n   *\n   * ```js\n   * // Fit a quadratic function by learning the coefficients a, b, c.\n   * const xs = tf.tensor1d([0, 1, 2, 3]);\n   * const ys = tf.tensor1d([1.1, 5.9, 16.8, 33.9]);\n   *\n   * const a = tf.scalar(Math.random()).variable();\n   * const b = tf.scalar(Math.random()).variable();\n   * const c = tf.scalar(Math.random()).variable();\n   *\n   * // y = a * x^2 + b * x + c.\n   * const f = x => a.mul(x.square()).add(b.mul(x)).add(c);\n   * const loss = (pred, label) => pred.sub(label).square().mean();\n   *\n   * const learningRate = 0.01;\n   * const optimizer = tf.train.sgd(learningRate);\n   *\n   * // Train the model.\n   * for (let i = 0; i < 10; i++) {\n   *   optimizer.minimize(() => loss(f(xs), ys));\n   * }\n   *\n   * // Make predictions.\n   * console.log(\n   *     `a: ${a.dataSync()}, b: ${b.dataSync()}, c: ${c.dataSync()}`);\n   * const preds = f(xs).dataSync();\n   * preds.forEach((pred, i) => {\n   *   console.log(`x: ${i}, pred: ${pred}`);\n   * });\n   * ```\n   *\n   * @param learningRate The learning rate to use for the SGD algorithm.\n   *\n   * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n   */\n  static sgd(learningRate) {\n    return new _sgd_optimizer.SGDOptimizer(learningRate);\n  }\n  /**\n   * Constructs a `tf.MomentumOptimizer` that uses momentum gradient\n   * descent.\n   *\n   * See\n   * [http://proceedings.mlr.press/v28/sutskever13.pdf](\n   * http://proceedings.mlr.press/v28/sutskever13.pdf)\n   *\n   * @param learningRate The learning rate to use for the Momentum gradient\n   * descent algorithm.\n   * @param momentum The momentum to use for the momentum gradient descent\n   * algorithm.\n   *\n   * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n   */\n\n\n  static momentum(learningRate, momentum, useNesterov = false) {\n    return new _momentum_optimizer.MomentumOptimizer(learningRate, momentum, useNesterov);\n  }\n  /**\n   * Constructs a `tf.RMSPropOptimizer` that uses RMSProp gradient\n   * descent. This implementation uses plain momentum and is not centered\n   * version of RMSProp.\n   *\n   * See\n   * [http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf](\n   * http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)\n   *\n   * @param learningRate The learning rate to use for the RMSProp gradient\n   * descent algorithm.\n   * @param decay The discounting factor for the history/coming gradient.\n   * @param momentum The momentum to use for the RMSProp gradient descent\n   * algorithm.\n   * @param epsilon Small value to avoid zero denominator.\n   * @param centered If true, gradients are normalized by the estimated\n   * variance of the gradient.\n   *\n   * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n   */\n\n\n  static rmsprop(learningRate, decay = .9, momentum = 0.0, epsilon = null, centered = false) {\n    return new _rmsprop_optimizer.RMSPropOptimizer(learningRate, decay, momentum, epsilon, centered);\n  }\n  /**\n   * Constructs a `tf.AdamOptimizer` that uses the Adam algorithm.\n   * See [https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)\n   *\n   * @param learningRate The learning rate to use for the Adam gradient\n   * descent algorithm.\n   * @param beta1 The exponential decay rate for the 1st moment estimates.\n   * @param beta2 The exponential decay rate for the 2nd moment estimates.\n   * @param epsilon A small constant for numerical stability.\n   *\n   * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n   */\n\n\n  static adam(learningRate = 0.001, beta1 = 0.9, beta2 = 0.999, epsilon = null) {\n    return new _adam_optimizer.AdamOptimizer(learningRate, beta1, beta2, epsilon);\n  }\n  /**\n   * Constructs a `tf.AdadeltaOptimizer` that uses the Adadelta algorithm.\n   * See [https://arxiv.org/abs/1212.5701](https://arxiv.org/abs/1212.5701)\n   *\n   * @param learningRate The learning rate to use for the Adadelta gradient\n   * descent algorithm.\n   * @param rho The learning rate decay over each update.\n   * @param epsilon A constant epsilon used to better condition the grad\n   * update.\n   *\n   * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n   */\n\n\n  static adadelta(learningRate = .001, rho = .95, epsilon = null) {\n    return new _adadelta_optimizer.AdadeltaOptimizer(learningRate, rho, epsilon);\n  }\n  /**\n   * Constructs a `tf.AdamaxOptimizer` that uses the Adamax algorithm.\n   * See [https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)\n   *\n   * @param learningRate The learning rate to use for the Adamax gradient\n   * descent algorithm.\n   * @param beta1 The exponential decay rate for the 1st moment estimates.\n   * @param beta2 The exponential decay rate for the 2nd moment estimates.\n   * @param epsilon A small constant for numerical stability.\n   * @param decay The learning rate decay over each update.\n   *\n   * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n   */\n\n\n  static adamax(learningRate = 0.002, beta1 = 0.9, beta2 = 0.999, epsilon = null, decay = 0.0) {\n    return new _adamax_optimizer.AdamaxOptimizer(learningRate, beta1, beta2, epsilon, decay);\n  }\n  /**\n   * Constructs a `tf.AdagradOptimizer` that uses the Adagrad algorithm.\n   * See\n   * [http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf](\n   * http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)\n   * or\n   * [http://ruder.io/optimizing-gradient-descent/index.html#adagrad](\n   * http://ruder.io/optimizing-gradient-descent/index.html#adagrad)\n   *\n   * @param learningRate The learning rate to use for the Adagrad gradient\n   * descent algorithm.\n   * @param initialAccumulatorValue Starting value for the accumulators, must be\n   * positive.\n   *\n   * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n   */\n\n\n  static adagrad(learningRate, initialAccumulatorValue = 0.1) {\n    return new _adagrad_optimizer.AdagradOptimizer(learningRate, initialAccumulatorValue);\n  }\n\n}\n\nexports.OptimizerConstructors = OptimizerConstructors;"},"sourceMaps":{"js":{"mappings":[{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":18,"column":0},"generated":{"line":8,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":19,"column":0},"generated":{"line":10,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":20,"column":0},"generated":{"line":12,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":21,"column":0},"generated":{"line":14,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":22,"column":0},"generated":{"line":16,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":23,"column":0},"generated":{"line":18,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":24,"column":0},"generated":{"line":20,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":1,"column":0},"generated":{"line":22,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":26,"column":6},"generated":{"line":38,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":26,"column":13},"generated":{"line":38,"column":6}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":26,"column":6},"generated":{"line":38,"column":27}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":26,"column":34},"generated":{"line":38,"column":28}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":27,"column":2},"generated":{"line":39,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":64,"column":2},"generated":{"line":76,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":64,"column":9},"generated":{"line":76,"column":9}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":64,"column":2},"generated":{"line":76,"column":12}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":64,"column":13},"generated":{"line":76,"column":13}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":64,"column":2},"generated":{"line":76,"column":25}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":64,"column":33},"generated":{"line":76,"column":27}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":65,"column":4},"generated":{"line":77,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":65,"column":11},"generated":{"line":77,"column":11}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":65,"column":15},"generated":{"line":77,"column":15}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":65,"column":11},"generated":{"line":77,"column":42}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":65,"column":28},"generated":{"line":77,"column":43}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":65,"column":11},"generated":{"line":77,"column":55}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":65,"column":4},"generated":{"line":77,"column":56}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":66,"column":3},"generated":{"line":78,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":68,"column":2},"generated":{"line":79,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":83,"column":2},"generated":{"line":96,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":83,"column":9},"generated":{"line":96,"column":9}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":83,"column":2},"generated":{"line":96,"column":17}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":83,"column":18},"generated":{"line":96,"column":18}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":83,"column":2},"generated":{"line":96,"column":30}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":83,"column":40},"generated":{"line":96,"column":32}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":83,"column":2},"generated":{"line":96,"column":40}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":83,"column":58},"generated":{"line":96,"column":42}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":83,"column":69},"generated":{"line":96,"column":53}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":83,"column":72},"generated":{"line":96,"column":56}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":83,"column":2},"generated":{"line":96,"column":61}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":83,"column":77},"generated":{"line":96,"column":63}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":85,"column":4},"generated":{"line":97,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":85,"column":11},"generated":{"line":97,"column":11}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":85,"column":15},"generated":{"line":97,"column":15}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":85,"column":11},"generated":{"line":97,"column":52}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":85,"column":33},"generated":{"line":97,"column":53}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":85,"column":11},"generated":{"line":97,"column":65}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":85,"column":47},"generated":{"line":97,"column":67}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":85,"column":11},"generated":{"line":97,"column":75}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":85,"column":57},"generated":{"line":97,"column":77}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":85,"column":11},"generated":{"line":97,"column":88}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":85,"column":4},"generated":{"line":97,"column":89}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":86,"column":3},"generated":{"line":98,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":88,"column":2},"generated":{"line":99,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":108,"column":2},"generated":{"line":121,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":108,"column":9},"generated":{"line":121,"column":9}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":108,"column":2},"generated":{"line":121,"column":16}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":109,"column":6},"generated":{"line":121,"column":17}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":108,"column":2},"generated":{"line":121,"column":29}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":109,"column":28},"generated":{"line":121,"column":31}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":109,"column":33},"generated":{"line":121,"column":36}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":109,"column":36},"generated":{"line":121,"column":39}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":108,"column":2},"generated":{"line":121,"column":41}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":109,"column":40},"generated":{"line":121,"column":43}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":109,"column":48},"generated":{"line":121,"column":51}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":109,"column":51},"generated":{"line":121,"column":54}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":108,"column":2},"generated":{"line":121,"column":57}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":109,"column":56},"generated":{"line":121,"column":59}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":109,"column":56},"generated":{"line":121,"column":66}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":109,"column":74},"generated":{"line":121,"column":69}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":108,"column":2},"generated":{"line":121,"column":73}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":110,"column":6},"generated":{"line":121,"column":75}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":110,"column":14},"generated":{"line":121,"column":83}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":110,"column":17},"generated":{"line":121,"column":86}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":108,"column":2},"generated":{"line":121,"column":91}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":110,"column":22},"generated":{"line":121,"column":93}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":111,"column":4},"generated":{"line":122,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":111,"column":11},"generated":{"line":122,"column":11}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":111,"column":15},"generated":{"line":122,"column":15}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":111,"column":11},"generated":{"line":122,"column":50}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":112,"column":8},"generated":{"line":122,"column":51}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":111,"column":11},"generated":{"line":122,"column":63}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":112,"column":22},"generated":{"line":122,"column":65}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":111,"column":11},"generated":{"line":122,"column":70}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":112,"column":29},"generated":{"line":122,"column":72}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":111,"column":11},"generated":{"line":122,"column":80}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":112,"column":39},"generated":{"line":122,"column":82}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":111,"column":11},"generated":{"line":122,"column":89}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":112,"column":48},"generated":{"line":122,"column":91}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":111,"column":11},"generated":{"line":122,"column":99}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":111,"column":4},"generated":{"line":122,"column":100}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":113,"column":3},"generated":{"line":123,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":115,"column":2},"generated":{"line":124,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":127,"column":2},"generated":{"line":138,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":127,"column":9},"generated":{"line":138,"column":9}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":127,"column":2},"generated":{"line":138,"column":13}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":128,"column":6},"generated":{"line":138,"column":14}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":128,"column":18},"generated":{"line":138,"column":26}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":128,"column":21},"generated":{"line":138,"column":29}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":127,"column":2},"generated":{"line":138,"column":34}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":128,"column":28},"generated":{"line":138,"column":36}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":128,"column":33},"generated":{"line":138,"column":41}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":128,"column":36},"generated":{"line":138,"column":44}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":127,"column":2},"generated":{"line":138,"column":47}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":128,"column":41},"generated":{"line":138,"column":49}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":128,"column":46},"generated":{"line":138,"column":54}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":128,"column":49},"generated":{"line":138,"column":57}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":127,"column":2},"generated":{"line":138,"column":62}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":129,"column":6},"generated":{"line":138,"column":64}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":129,"column":6},"generated":{"line":138,"column":71}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":129,"column":24},"generated":{"line":138,"column":74}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":127,"column":2},"generated":{"line":138,"column":78}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":129,"column":28},"generated":{"line":138,"column":80}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":130,"column":4},"generated":{"line":139,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":130,"column":11},"generated":{"line":139,"column":11}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":130,"column":15},"generated":{"line":139,"column":15}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":130,"column":11},"generated":{"line":139,"column":44}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":130,"column":29},"generated":{"line":139,"column":45}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":130,"column":11},"generated":{"line":139,"column":57}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":130,"column":43},"generated":{"line":139,"column":59}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":130,"column":11},"generated":{"line":139,"column":64}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":130,"column":50},"generated":{"line":139,"column":66}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":130,"column":11},"generated":{"line":139,"column":71}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":130,"column":57},"generated":{"line":139,"column":73}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":130,"column":11},"generated":{"line":139,"column":80}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":130,"column":4},"generated":{"line":139,"column":81}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":131,"column":3},"generated":{"line":140,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":133,"column":2},"generated":{"line":141,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":145,"column":2},"generated":{"line":155,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":145,"column":9},"generated":{"line":155,"column":9}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":145,"column":2},"generated":{"line":155,"column":17}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":145,"column":18},"generated":{"line":155,"column":18}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":145,"column":30},"generated":{"line":155,"column":30}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":145,"column":33},"generated":{"line":155,"column":33}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":145,"column":2},"generated":{"line":155,"column":37}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":145,"column":39},"generated":{"line":155,"column":39}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":145,"column":42},"generated":{"line":155,"column":42}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":145,"column":45},"generated":{"line":155,"column":45}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":145,"column":2},"generated":{"line":155,"column":48}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":145,"column":50},"generated":{"line":155,"column":50}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":145,"column":50},"generated":{"line":155,"column":57}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":145,"column":68},"generated":{"line":155,"column":60}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":145,"column":2},"generated":{"line":155,"column":64}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":145,"column":72},"generated":{"line":155,"column":66}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":147,"column":4},"generated":{"line":156,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":147,"column":11},"generated":{"line":156,"column":11}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":147,"column":15},"generated":{"line":156,"column":15}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":147,"column":11},"generated":{"line":156,"column":52}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":147,"column":33},"generated":{"line":156,"column":53}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":147,"column":11},"generated":{"line":156,"column":65}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":147,"column":47},"generated":{"line":156,"column":67}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":147,"column":11},"generated":{"line":156,"column":70}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":147,"column":52},"generated":{"line":156,"column":72}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":147,"column":11},"generated":{"line":156,"column":79}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":147,"column":4},"generated":{"line":156,"column":80}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":148,"column":3},"generated":{"line":157,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":150,"column":2},"generated":{"line":158,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":163,"column":2},"generated":{"line":173,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":163,"column":9},"generated":{"line":173,"column":9}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":163,"column":2},"generated":{"line":173,"column":15}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":164,"column":6},"generated":{"line":173,"column":16}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":164,"column":18},"generated":{"line":173,"column":28}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":164,"column":21},"generated":{"line":173,"column":31}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":163,"column":2},"generated":{"line":173,"column":36}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":164,"column":28},"generated":{"line":173,"column":38}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":164,"column":33},"generated":{"line":173,"column":43}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":164,"column":36},"generated":{"line":173,"column":46}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":163,"column":2},"generated":{"line":173,"column":49}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":164,"column":41},"generated":{"line":173,"column":51}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":164,"column":46},"generated":{"line":173,"column":56}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":164,"column":49},"generated":{"line":173,"column":59}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":163,"column":2},"generated":{"line":173,"column":64}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":164,"column":56},"generated":{"line":173,"column":66}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":164,"column":56},"generated":{"line":173,"column":73}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":164,"column":74},"generated":{"line":173,"column":76}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":163,"column":2},"generated":{"line":173,"column":80}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":165,"column":6},"generated":{"line":173,"column":82}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":165,"column":11},"generated":{"line":173,"column":87}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":165,"column":14},"generated":{"line":173,"column":90}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":163,"column":2},"generated":{"line":173,"column":93}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":165,"column":17},"generated":{"line":173,"column":95}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":166,"column":4},"generated":{"line":174,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":166,"column":11},"generated":{"line":174,"column":11}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":166,"column":15},"generated":{"line":174,"column":15}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":166,"column":11},"generated":{"line":174,"column":48}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":166,"column":31},"generated":{"line":174,"column":49}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":166,"column":11},"generated":{"line":174,"column":61}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":166,"column":45},"generated":{"line":174,"column":63}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":166,"column":11},"generated":{"line":174,"column":68}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":166,"column":52},"generated":{"line":174,"column":70}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":166,"column":11},"generated":{"line":174,"column":75}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":166,"column":59},"generated":{"line":174,"column":77}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":166,"column":11},"generated":{"line":174,"column":84}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":166,"column":68},"generated":{"line":174,"column":86}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":166,"column":11},"generated":{"line":174,"column":91}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":166,"column":4},"generated":{"line":174,"column":92}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":167,"column":3},"generated":{"line":175,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":169,"column":2},"generated":{"line":176,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":185,"column":2},"generated":{"line":194,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":185,"column":9},"generated":{"line":194,"column":9}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":185,"column":2},"generated":{"line":194,"column":16}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":185,"column":17},"generated":{"line":194,"column":17}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":185,"column":2},"generated":{"line":194,"column":29}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":185,"column":39},"generated":{"line":194,"column":31}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":185,"column":62},"generated":{"line":194,"column":54}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":185,"column":65},"generated":{"line":194,"column":57}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":185,"column":2},"generated":{"line":194,"column":60}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":185,"column":68},"generated":{"line":194,"column":62}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":187,"column":4},"generated":{"line":195,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":187,"column":11},"generated":{"line":195,"column":11}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":187,"column":15},"generated":{"line":195,"column":15}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":187,"column":11},"generated":{"line":195,"column":50}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":187,"column":32},"generated":{"line":195,"column":51}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":187,"column":11},"generated":{"line":195,"column":63}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":187,"column":46},"generated":{"line":195,"column":65}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":187,"column":11},"generated":{"line":195,"column":88}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":187,"column":4},"generated":{"line":195,"column":89}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":188,"column":3},"generated":{"line":196,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":26,"column":34},"generated":{"line":198,"column":0}}],"sources":{"../../src/optimizers/optimizer_constructors.ts":"/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {AdadeltaOptimizer} from './adadelta_optimizer';\nimport {AdagradOptimizer} from './adagrad_optimizer';\nimport {AdamOptimizer} from './adam_optimizer';\nimport {AdamaxOptimizer} from './adamax_optimizer';\nimport {MomentumOptimizer} from './momentum_optimizer';\nimport {RMSPropOptimizer} from './rmsprop_optimizer';\nimport {SGDOptimizer} from './sgd_optimizer';\n\nexport class OptimizerConstructors {\n  /**\n   * Constructs a `tf.SGDOptimizer` that uses stochastic gradient descent.\n   *\n   * ```js\n   * // Fit a quadratic function by learning the coefficients a, b, c.\n   * const xs = tf.tensor1d([0, 1, 2, 3]);\n   * const ys = tf.tensor1d([1.1, 5.9, 16.8, 33.9]);\n   *\n   * const a = tf.scalar(Math.random()).variable();\n   * const b = tf.scalar(Math.random()).variable();\n   * const c = tf.scalar(Math.random()).variable();\n   *\n   * // y = a * x^2 + b * x + c.\n   * const f = x => a.mul(x.square()).add(b.mul(x)).add(c);\n   * const loss = (pred, label) => pred.sub(label).square().mean();\n   *\n   * const learningRate = 0.01;\n   * const optimizer = tf.train.sgd(learningRate);\n   *\n   * // Train the model.\n   * for (let i = 0; i < 10; i++) {\n   *   optimizer.minimize(() => loss(f(xs), ys));\n   * }\n   *\n   * // Make predictions.\n   * console.log(\n   *     `a: ${a.dataSync()}, b: ${b.dataSync()}, c: ${c.dataSync()}`);\n   * const preds = f(xs).dataSync();\n   * preds.forEach((pred, i) => {\n   *   console.log(`x: ${i}, pred: ${pred}`);\n   * });\n   * ```\n   *\n   * @param learningRate The learning rate to use for the SGD algorithm.\n   *\n   * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n   */\n  static sgd(learningRate: number): SGDOptimizer {\n    return new SGDOptimizer(learningRate);\n  }\n\n  /**\n   * Constructs a `tf.MomentumOptimizer` that uses momentum gradient\n   * descent.\n   *\n   * See\n   * [http://proceedings.mlr.press/v28/sutskever13.pdf](\n   * http://proceedings.mlr.press/v28/sutskever13.pdf)\n   *\n   * @param learningRate The learning rate to use for the Momentum gradient\n   * descent algorithm.\n   * @param momentum The momentum to use for the momentum gradient descent\n   * algorithm.\n   *\n   * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n   */\n  static momentum(learningRate: number, momentum: number, useNesterov = false):\n      MomentumOptimizer {\n    return new MomentumOptimizer(learningRate, momentum, useNesterov);\n  }\n\n  /**\n   * Constructs a `tf.RMSPropOptimizer` that uses RMSProp gradient\n   * descent. This implementation uses plain momentum and is not centered\n   * version of RMSProp.\n   *\n   * See\n   * [http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf](\n   * http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)\n   *\n   * @param learningRate The learning rate to use for the RMSProp gradient\n   * descent algorithm.\n   * @param decay The discounting factor for the history/coming gradient.\n   * @param momentum The momentum to use for the RMSProp gradient descent\n   * algorithm.\n   * @param epsilon Small value to avoid zero denominator.\n   * @param centered If true, gradients are normalized by the estimated\n   * variance of the gradient.\n   *\n   * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n   */\n  static rmsprop(\n      learningRate: number, decay = .9, momentum = 0.0, epsilon: number = null,\n      centered = false): RMSPropOptimizer {\n    return new RMSPropOptimizer(\n        learningRate, decay, momentum, epsilon, centered);\n  }\n\n  /**\n   * Constructs a `tf.AdamOptimizer` that uses the Adam algorithm.\n   * See [https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)\n   *\n   * @param learningRate The learning rate to use for the Adam gradient\n   * descent algorithm.\n   * @param beta1 The exponential decay rate for the 1st moment estimates.\n   * @param beta2 The exponential decay rate for the 2nd moment estimates.\n   * @param epsilon A small constant for numerical stability.\n   *\n   * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n   */\n  static adam(\n      learningRate = 0.001, beta1 = 0.9, beta2 = 0.999,\n      epsilon: number = null): AdamOptimizer {\n    return new AdamOptimizer(learningRate, beta1, beta2, epsilon);\n  }\n\n  /**\n   * Constructs a `tf.AdadeltaOptimizer` that uses the Adadelta algorithm.\n   * See [https://arxiv.org/abs/1212.5701](https://arxiv.org/abs/1212.5701)\n   *\n   * @param learningRate The learning rate to use for the Adadelta gradient\n   * descent algorithm.\n   * @param rho The learning rate decay over each update.\n   * @param epsilon A constant epsilon used to better condition the grad\n   * update.\n   *\n   * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n   */\n  static adadelta(learningRate = .001, rho = .95, epsilon: number = null):\n      AdadeltaOptimizer {\n    return new AdadeltaOptimizer(learningRate, rho, epsilon);\n  }\n\n  /**\n   * Constructs a `tf.AdamaxOptimizer` that uses the Adamax algorithm.\n   * See [https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)\n   *\n   * @param learningRate The learning rate to use for the Adamax gradient\n   * descent algorithm.\n   * @param beta1 The exponential decay rate for the 1st moment estimates.\n   * @param beta2 The exponential decay rate for the 2nd moment estimates.\n   * @param epsilon A small constant for numerical stability.\n   * @param decay The learning rate decay over each update.\n   *\n   * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n   */\n  static adamax(\n      learningRate = 0.002, beta1 = 0.9, beta2 = 0.999, epsilon: number = null,\n      decay = 0.0): AdamaxOptimizer {\n    return new AdamaxOptimizer(learningRate, beta1, beta2, epsilon, decay);\n  }\n\n  /**\n   * Constructs a `tf.AdagradOptimizer` that uses the Adagrad algorithm.\n   * See\n   * [http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf](\n   * http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)\n   * or\n   * [http://ruder.io/optimizing-gradient-descent/index.html#adagrad](\n   * http://ruder.io/optimizing-gradient-descent/index.html#adagrad)\n   *\n   * @param learningRate The learning rate to use for the Adagrad gradient\n   * descent algorithm.\n   * @param initialAccumulatorValue Starting value for the accumulators, must be\n   * positive.\n   *\n   * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n   */\n  static adagrad(learningRate: number, initialAccumulatorValue = 0.1):\n      AdagradOptimizer {\n    return new AdagradOptimizer(learningRate, initialAccumulatorValue);\n  }\n}\n"},"lineCount":null}},"error":null,"hash":"d185c5529c246e8fdb9c8086c15b3f3f","cacheData":{"env":{}}}